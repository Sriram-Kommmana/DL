

    import numpy as np
    import pandas as pd
    from sklearn.datasets import make_moons

    X, y = make_moons(
        n_samples=1000,
        noise=0.2,
        random_state=42
    )

    print("X shape:", X.shape)
    print("y shape:", y.shape)

    X shape: (1000, 2)
    y shape: (1000,)

    df = pd.DataFrame(X, columns=["x1", "x2"])
    df["label"] = y
    df.to_csv("moons.csv", index=False)

    print("moons.csv saved successfully")

    moons.csv saved successfully

    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import StandardScaler

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    import matplotlib.pyplot as plt
    from sklearn.datasets import make_moons

    # Generate dataset
    X, y = make_moons(
        n_samples=1000,
        noise=0.2,
        random_state=42
    )

    # Plot
    plt.figure(figsize=(6, 6))
    plt.scatter(
        X[y == 0][:, 0], X[y == 0][:, 1],
        label="Class 0", alpha=0.7
    )
    plt.scatter(
        X[y == 1][:, 0], X[y == 1][:, 1],
        label="Class 1", alpha=0.7
    )

    plt.xlabel("Feature 1")
    plt.ylabel("Feature 2")
    plt.title("Two-Moons Dataset Visualization")
    plt.legend()
    plt.grid(True)
    plt.show()

[]

    import torch
    import torch.nn as nn
    import torch.optim as optim
    from torch.utils.data import TensorDataset, DataLoader

    X_train_t = torch.tensor(X_train, dtype=torch.float32)
    y_train_t = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)
    X_test_t = torch.tensor(X_test, dtype=torch.float32)
    y_test_t = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)

    train_loader = DataLoader(
        TensorDataset(X_train_t, y_train_t),
        batch_size=32, shuffle=True
    )

    class MoonNet(nn.Module):
        def __init__(self, activation):
            super().__init__()
            self.fc1 = nn.Linear(2, 16)
            self.fc2 = nn.Linear(16, 16)
            self.fc3 = nn.Linear(16, 1)
            self.act = activation

        def forward(self, x):
            x = self.act(self.fc1(x))
            x = self.act(self.fc2(x))
            return torch.sigmoid(self.fc3(x))

    def train_model(activation_fn, lr=0.001, epochs=50):
        model = MoonNet(activation_fn)
        criterion = nn.BCELoss()
        optimizer = optim.Adam(model.parameters(), lr=lr)

        losses = []
        initial_loss = None

        for epoch in range(epochs):
            epoch_loss = 0
            for xb, yb in train_loader:
                optimizer.zero_grad()
                preds = model(xb)
                loss = criterion(preds, yb)
                loss.backward()
                optimizer.step()
                epoch_loss += loss.item()

            epoch_loss /= len(train_loader)
            losses.append(epoch_loss)

            if epoch == 0:
                initial_loss = epoch_loss

        def accuracy(x, y):
            with torch.no_grad():
                preds = (model(x) > 0.5).float()
                return (preds.eq(y).sum() / y.shape[0]).item()

        return {
            "initial_loss": initial_loss,
            "train_acc": accuracy(X_train_t, y_train_t),
            "test_acc": accuracy(X_test_t, y_test_t),
            "losses": losses
        }

##EXERCISE 1 - Comparative Study of Activation Functions

    activations = {
        "Sigmoid": nn.Sigmoid(),
        "Tanh": nn.Tanh(),
        "ReLU": nn.ReLU()
    }

    results_pt = {}
    for name, act in activations.items():
        results_pt[name] = train_model(act)

    results_pt

    {'Sigmoid': {'initial_loss': 0.7087104177474975,
      'train_acc': 0.8687499761581421,
      'test_acc': 0.8500000238418579,
      'losses': [0.7087104177474975,
       0.6892985105514526,
       0.6799876642227173,
       0.6716880869865417,
       0.6620098638534546,
       0.6494675588607788,
       0.6333447980880738,
       0.6135891079902649,
       0.5892219877243042,
       0.5620854616165161,
       0.5322116482257843,
       0.5013870012760162,
       0.472157164812088,
       0.445721333026886,
       0.42212716341018675,
       0.4027480638027191,
       0.3867565965652466,
       0.37295880675315857,
       0.3622904908657074,
       0.35328493475914,
       0.3460086572170258,
       0.3398260724544525,
       0.3346986275911331,
       0.33070878565311435,
       0.327095599770546,
       0.3240725541114807,
       0.32248233020305633,
       0.3197904342412949,
       0.31781013488769533,
       0.31693270683288577,
       0.31525089621543884,
       0.31428750514984133,
       0.3132315218448639,
       0.3123438769578934,
       0.31191892504692076,
       0.3111959648132324,
       0.31096806228160856,
       0.3106838369369507,
       0.3100996828079224,
       0.30929952502250674,
       0.30920715928077697,
       0.3087804734706879,
       0.308278186917305,
       0.3084200745820999,
       0.30767192840576174,
       0.307563493847847,
       0.3072041940689087,
       0.3070902770757675,
       0.30664645075798036,
       0.3065149283409119]},
     'Tanh': {'initial_loss': 0.6289431095123291,
      'train_acc': 0.887499988079071,
      'test_acc': 0.8849999904632568,
      'losses': [0.6289431095123291,
       0.530586165189743,
       0.44377806663513186,
       0.38094848036766055,
       0.34469410836696623,
       0.3249477314949036,
       0.3157868593931198,
       0.31135758876800534,
       0.3078214466571808,
       0.30606014966964723,
       0.30519034326076505,
       0.30399301648139954,
       0.3032297432422638,
       0.30237812459468844,
       0.30162828266620634,
       0.3011263352632523,
       0.3004688322544098,
       0.30016473710536956,
       0.29908365666866304,
       0.29875106662511824,
       0.29770918905735017,
       0.2970859777927399,
       0.2966034612059593,
       0.2959086871147156,
       0.295212140083313,
       0.2946268332004547,
       0.29456501007080077,
       0.2934404057264328,
       0.2926267129182816,
       0.2919978535175323,
       0.2915049773454666,
       0.29080808103084566,
       0.2907492259144783,
       0.2901944428682327,
       0.28809822142124175,
       0.2874069672822952,
       0.286256345808506,
       0.28578203558921816,
       0.284790472984314,
       0.2834822472929954,
       0.2841318106651306,
       0.2807630518078804,
       0.2802359944581985,
       0.278984634578228,
       0.2761485993862152,
       0.27513128697872163,
       0.2723940175771713,
       0.2715886244177818,
       0.2693216812610626,
       0.266459002494812]},
     'ReLU': {'initial_loss': 0.6893299174308777,
      'train_acc': 0.918749988079071,
      'test_acc': 0.9150000214576721,
      'losses': [0.6893299174308777,
       0.6395959210395813,
       0.5753838491439819,
       0.504076316356659,
       0.43944610834121706,
       0.38958988070487977,
       0.3559907633066177,
       0.33299784898757934,
       0.3184163397550583,
       0.30914289236068726,
       0.30246526837348936,
       0.2982886743545532,
       0.2950215548276901,
       0.2920157301425934,
       0.2912188771367073,
       0.2880708122253418,
       0.28571368038654327,
       0.2839438319206238,
       0.282250257730484,
       0.28056794226169585,
       0.27920205891132355,
       0.27785061597824096,
       0.2756270796060562,
       0.2735950154066086,
       0.27184994995594025,
       0.2697279208898544,
       0.26773511707782743,
       0.2655104550719261,
       0.2631581795215607,
       0.2618094089627266,
       0.2588677781820297,
       0.2564947488903999,
       0.2539234530925751,
       0.2520537582039833,
       0.24906361490488052,
       0.24650645941495897,
       0.2438534054160118,
       0.24115937769412996,
       0.2382814395427704,
       0.23561007142066956,
       0.2339316862821579,
       0.2297090619802475,
       0.2271407088637352,
       0.22379752188920976,
       0.2211833992600441,
       0.2176089772582054,
       0.2150903907418251,
       0.21153089076280593,
       0.20769205957651138,
       0.20436566561460495]}}

    import matplotlib.pyplot as plt

    plt.figure()
    for name, res in results_pt.items():
        plt.plot(res["losses"], label=name)

    plt.xlabel("Epoch")
    plt.ylabel("Training Loss")
    plt.title("PyTorch: Loss Curves for Different Activations")
    plt.legend()
    plt.grid(True)
    plt.show()

[]

    import pandas as pd

    pt_table = pd.DataFrame([
        {
            "Activation": name,
            "Initial Loss": round(res["initial_loss"], 4),
            "Final Train Accuracy": round(res["train_acc"], 4),
            "Final Test Accuracy": round(res["test_acc"], 4)
        }
        for name, res in results_pt.items()
    ])

    pt_table

    {"summary":"{\n  \"name\": \"pt_table\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"Activation\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Sigmoid\",\n          \"Tanh\",\n          \"ReLU\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Initial Loss\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04161842540670337,\n        \"min\": 0.6289,\n        \"max\": 0.7087,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.7087,\n          0.6289,\n          0.6893\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Final Train Accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.025254966508259948,\n        \"min\": 0.8687,\n        \"max\": 0.9187,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.8687,\n          0.8875,\n          0.9187\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Final Test Accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.03253203549323859,\n        \"min\": 0.85,\n        \"max\": 0.915,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.85,\n          0.885,\n          0.915\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}","type":"dataframe","variable_name":"pt_table"}

##EXERCISE 2 - Effect of Activation Function on Convergence Speed

    def convergence_epoch(losses, threshold=0.001, patience=5):
        for i in range(len(losses) - patience):
            diffs = [abs(losses[i+j] - losses[i+j+1]) for j in range(patience)]
            if all(d < threshold for d in diffs):
                return i + 1
        return None

    for name, res in results_pt.items():
        print(
            name,
            "Converges at epoch:",
            convergence_epoch(res["losses"]),
            "| Loss Curve:",
            "Smooth" if name != "ReLU" else "Fast"
        )

    Sigmoid Converges at epoch: 33 | Loss Curve: Smooth
    Tanh Converges at epoch: 12 | Loss Curve: Smooth
    ReLU Converges at epoch: None | Loss Curve: Fast

##EXERCISE 3

    learning_rates = [0.0005, 0.01]

    lr_results_pt = {}

    for name, act in activations.items():
        for lr in learning_rates:
            key = f"{name}_LR_{lr}"
            lr_results_pt[key] = train_model(act, lr=lr)

    lr_results_pt

    {'Sigmoid_LR_0.0005': {'initial_loss': 0.6915115165710449,
      'train_acc': 0.8662499785423279,
      'test_acc': 0.8550000190734863,
      'losses': [0.6915115165710449,
       0.684760971069336,
       0.6794254636764526,
       0.6751950979232788,
       0.6694665431976319,
       0.6635963678359985,
       0.6568624520301819,
       0.6492816257476807,
       0.6405009055137634,
       0.6305648827552796,
       0.6193668508529663,
       0.6071121597290039,
       0.5938764333724975,
       0.5790307188034057,
       0.5634604358673095,
       0.5471752452850341,
       0.530383141040802,
       0.5133248901367188,
       0.4965221929550171,
       0.4797637641429901,
       0.4634858214855194,
       0.4483191680908203,
       0.4342675578594208,
       0.42128116607666016,
       0.4090516626834869,
       0.39839870810508726,
       0.38847784519195555,
       0.37970935702323916,
       0.3719685411453247,
       0.3648540824651718,
       0.35871055364608767,
       0.3533699524402618,
       0.34811077892780307,
       0.3438181698322296,
       0.3399579983949661,
       0.336217405796051,
       0.333243163228035,
       0.3303469920158386,
       0.32762460529804227,
       0.32553411185741427,
       0.32322572350502016,
       0.32149586856365203,
       0.3197427064180374,
       0.31812355279922483,
       0.31671812176704406,
       0.3156258100271225,
       0.314267560839653,
       0.31317604064941407,
       0.31233263194561006,
       0.3116612505912781]},
     'Sigmoid_LR_0.01': {'initial_loss': 0.6565968728065491,
      'train_acc': 0.9524999856948853,
      'test_acc': 0.9649999737739563,
      'losses': [0.6565968728065491,
       0.47126011967659,
       0.33385724186897275,
       0.30917956739664076,
       0.3055499517917633,
       0.31234628915786744,
       0.3051011335849762,
       0.30417378902435305,
       0.3027285414934158,
       0.30255025416612624,
       0.30197973400354383,
       0.30432643353939054,
       0.2999908822774887,
       0.29689252853393555,
       0.3061184465885162,
       0.29286711633205414,
       0.28509708762168884,
       0.2805382627248764,
       0.2669862997531891,
       0.2592538568377495,
       0.24376888185739518,
       0.23596659421920776,
       0.21721548825502396,
       0.2011397585272789,
       0.18905532211065293,
       0.17383000403642654,
       0.16205552309751511,
       0.15238115504384042,
       0.14238332241773605,
       0.13470238864421843,
       0.12728635847568512,
       0.12681677117943763,
       0.12105014733970165,
       0.11814127132296562,
       0.11471190825104713,
       0.11169937118887902,
       0.11821932330727578,
       0.11158529669046402,
       0.10736678503453731,
       0.10369940645992756,
       0.10737420469522477,
       0.10476137287914752,
       0.10371552132070065,
       0.10115112073719501,
       0.10254933156073093,
       0.10327828422188759,
       0.09881176307797432,
       0.09957304000854492,
       0.09966282583773137,
       0.10310591533780097]},
     'Tanh_LR_0.0005': {'initial_loss': 0.6752277731895446,
      'train_acc': 0.8849999904632568,
      'test_acc': 0.8849999904632568,
      'losses': [0.6752277731895446,
       0.6308998727798462,
       0.5897049927711486,
       0.548480613231659,
       0.5084777307510376,
       0.46937034010887146,
       0.4347044289112091,
       0.404622505903244,
       0.3808825159072876,
       0.36182191729545593,
       0.347059885263443,
       0.3358309817314148,
       0.3273900705575943,
       0.32081746399402616,
       0.315924226641655,
       0.31233420193195344,
       0.3093368738889694,
       0.3074670910835266,
       0.3055891442298889,
       0.3042732667922974,
       0.3034683525562286,
       0.3024066972732544,
       0.3014530259370804,
       0.3005365002155304,
       0.3000685852766037,
       0.2991166964173317,
       0.2986252999305725,
       0.298112565279007,
       0.29709827542304995,
       0.29630323708057404,
       0.2957209551334381,
       0.2947358340024948,
       0.2939237406849861,
       0.2931253281235695,
       0.292293147444725,
       0.29129195839166644,
       0.29055567771196367,
       0.2892843359708786,
       0.28881188154220583,
       0.2870843806862831,
       0.2859566080570221,
       0.28496625304222106,
       0.2838027191162109,
       0.2821232622861862,
       0.2814246851205826,
       0.2792939281463623,
       0.278135638833046,
       0.2768512272834778,
       0.2747478574514389,
       0.2727869325876236]},
     'Tanh_LR_0.01': {'initial_loss': 0.46318238496780395,
      'train_acc': 0.9587500095367432,
      'test_acc': 0.9750000238418579,
      'losses': [0.46318238496780395,
       0.316515092253685,
       0.3047559070587158,
       0.2883844190835953,
       0.25814392030239103,
       0.2000670975446701,
       0.14289162307977676,
       0.12080877386033535,
       0.13248979270458222,
       0.10052543178200722,
       0.09609800815582276,
       0.09310313016176223,
       0.09060137838125229,
       0.10521477095782757,
       0.10238544557243585,
       0.09210155788809062,
       0.09312366031110286,
       0.10373555887490511,
       0.09872808016836643,
       0.10545627228915691,
       0.08894167620688677,
       0.09034066952764988,
       0.10032692439854145,
       0.09231405407190323,
       0.08873453341424466,
       0.09012429919093848,
       0.09410584285855293,
       0.09858306378126144,
       0.10100778691470623,
       0.09075029730796814,
       0.09933448649942875,
       0.08872648879885674,
       0.09533298503607511,
       0.09214711271226406,
       0.08920477502048016,
       0.10236917790025472,
       0.09328167028725147,
       0.08747064493596554,
       0.10737810423597693,
       0.10577475149184465,
       0.10425079075619578,
       0.09393543295562268,
       0.08998526860028505,
       0.08705757349729538,
       0.08856268107891083,
       0.09527365103363991,
       0.09332219753414392,
       0.08648702673614025,
       0.08709746621549129,
       0.10827157385647297]},
     'ReLU_LR_0.0005': {'initial_loss': 0.6673976969718933,
      'train_acc': 0.9387500286102295,
      'test_acc': 0.9449999928474426,
      'losses': [0.6673976969718933,
       0.6513162684440613,
       0.6316634058952332,
       0.6068472266197205,
       0.5755362010002136,
       0.5383623313903808,
       0.4979488229751587,
       0.4554591715335846,
       0.41498523712158203,
       0.37939193844795227,
       0.3494925034046173,
       0.3255057656764984,
       0.30707996487617495,
       0.2936668401956558,
       0.2828513073921204,
       0.275168134868145,
       0.2689651423692703,
       0.2643958079814911,
       0.2599219274520874,
       0.256287784576416,
       0.2529718953371048,
       0.24993513464927675,
       0.24713582843542098,
       0.24423408687114714,
       0.24146076381206513,
       0.23867227017879486,
       0.23577708274126052,
       0.23312442600727082,
       0.23023339301347734,
       0.22744098484516143,
       0.2248592919111252,
       0.2214043727517128,
       0.21844672799110412,
       0.21573068886995317,
       0.21260048776865006,
       0.20941439807415008,
       0.20633170753717422,
       0.20294302105903625,
       0.19979180455207823,
       0.19657860696315765,
       0.19335108369588852,
       0.1898670780658722,
       0.1868044875562191,
       0.18346245348453522,
       0.18043873429298402,
       0.17722768306732178,
       0.1740306881070137,
       0.17118347316980362,
       0.1680241885781288,
       0.16534683272242545]},
     'ReLU_LR_0.01': {'initial_loss': 0.4801528924703598,
      'train_acc': 0.9775000214576721,
      'test_acc': 0.9800000190734863,
      'losses': [0.4801528924703598,
       0.2908877146244049,
       0.25549650818109515,
       0.22160113215446473,
       0.18987251430749894,
       0.13729459524154664,
       0.1104587921500206,
       0.09445023752748966,
       0.08516351744532585,
       0.08285295449197293,
       0.08790540598332881,
       0.07820084167644381,
       0.0834084509126842,
       0.07993152312934398,
       0.07669787876307964,
       0.07607149340212345,
       0.0743596625328064,
       0.0714994726702571,
       0.07079951653257012,
       0.07214523512870073,
       0.07058733139187097,
       0.07282313410192728,
       0.06632505465298891,
       0.07552601013332605,
       0.07887077558785677,
       0.07172475811094045,
       0.0791806701105088,
       0.08176926443353295,
       0.06732985369861126,
       0.06300219956785441,
       0.0752850353717804,
       0.06686039883643388,
       0.06557332929223776,
       0.06814059752970934,
       0.06914874561131,
       0.06744240257889032,
       0.06684512697160244,
       0.06606461558490992,
       0.07096437178552151,
       0.06590423161163926,
       0.06982302374672145,
       0.07236853316426277,
       0.06990981575101614,
       0.07267072837799787,
       0.0675686301663518,
       0.06274231888353825,
       0.06774313868023456,
       0.0661227691732347,
       0.07513631280511618,
       0.06510567635297776]}}

    pt_lr_table = pd.DataFrame([
        {
            "Activation + LR": key,
            "Initial Loss": round(res["initial_loss"], 4),
            "Final Train Accuracy": round(res["train_acc"], 4),
            "Final Test Accuracy": round(res["test_acc"], 4)
        }
        for key, res in lr_results_pt.items()
    ])

    pt_lr_table

    {"summary":"{\n  \"name\": \"pt_lr_table\",\n  \"rows\": 6,\n  \"fields\": [\n    {\n      \"column\": \"Activation + LR\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"Sigmoid_LR_0.0005\",\n          \"Sigmoid_LR_0.01\",\n          \"ReLU_LR_0.01\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Initial Loss\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.10454289869076075,\n        \"min\": 0.4632,\n        \"max\": 0.6915,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0.6915,\n          0.6566,\n          0.4802\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Final Train Accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04419180919582272,\n        \"min\": 0.8662,\n        \"max\": 0.9775,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0.8662,\n          0.9525,\n          0.9775\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Final Test Accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05200160253940898,\n        \"min\": 0.855,\n        \"max\": 0.98,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0.855,\n          0.965,\n          0.98\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}","type":"dataframe","variable_name":"pt_lr_table"}

#KERAS (TENSORFLOW) IMPLEMENTATION

    import tensorflow as tf
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import Dense
    from tensorflow.keras.optimizers import Adam

    def build_model(activation, lr):
        model = Sequential([
            Dense(16, activation=activation, input_shape=(2,)),
            Dense(16, activation=activation),
            Dense(1, activation="sigmoid")
        ])
        model.compile(
            optimizer=Adam(learning_rate=lr),
            loss="binary_crossentropy",
            metrics=["accuracy"]
        )
        return model

    def train_keras(activation, lr=0.001, epochs=50):
        model = build_model(activation, lr)
        history = model.fit(
            X_train, y_train,
            validation_data=(X_test, y_test),
            epochs=epochs,
            verbose=0
        )

        return {
            "initial_loss": history.history["loss"][0],
            "train_acc": history.history["accuracy"][-1],
            "test_acc": history.history["val_accuracy"][-1],
            "losses": history.history["loss"]
        }

##EXERCISE 1 - Comparative Study of Activation Functions

    activations_keras = ["sigmoid", "tanh", "relu"]
    results_keras = {}

    for act in activations_keras:
        results_keras[act] = train_keras(act)

    results_keras

    /usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
      super().__init__(activity_regularizer=activity_regularizer, **kwargs)

    {'sigmoid': {'initial_loss': 0.6704892516136169,
      'train_acc': 0.8700000047683716,
      'test_acc': 0.8650000095367432,
      'losses': [0.6704892516136169,
       0.6538225412368774,
       0.6375462412834167,
       0.6179816126823425,
       0.5959389209747314,
       0.5712168216705322,
       0.5443246960639954,
       0.5153292417526245,
       0.48712149262428284,
       0.458683043718338,
       0.4327673614025116,
       0.4098111093044281,
       0.3895702660083771,
       0.3729358911514282,
       0.35911500453948975,
       0.34829843044281006,
       0.3393731415271759,
       0.332021027803421,
       0.326043963432312,
       0.321748822927475,
       0.3181024491786957,
       0.31453847885131836,
       0.3115288317203522,
       0.30950599908828735,
       0.3076882064342499,
       0.30615565180778503,
       0.3049350380897522,
       0.3042871057987213,
       0.3031609058380127,
       0.30233705043792725,
       0.3017379641532898,
       0.3012479841709137,
       0.30071038007736206,
       0.3004551827907562,
       0.3007008135318756,
       0.2999851703643799,
       0.2997484803199768,
       0.29944908618927,
       0.2995741665363312,
       0.29889896512031555,
       0.2992108464241028,
       0.29899102449417114,
       0.2987140119075775,
       0.2991504371166229,
       0.29824382066726685,
       0.29851242899894714,
       0.29844456911087036,
       0.2983853816986084,
       0.2984260022640228,
       0.2983989417552948]},
     'tanh': {'initial_loss': 0.9873805046081543,
      'train_acc': 0.9162499904632568,
      'test_acc': 0.9200000166893005,
      'losses': [0.9873805046081543,
       0.7192481160163879,
       0.5373732447624207,
       0.42924270033836365,
       0.3722185492515564,
       0.3413219451904297,
       0.3254029452800751,
       0.31684955954551697,
       0.31160736083984375,
       0.3076556324958801,
       0.30555230379104614,
       0.3040178120136261,
       0.3025995194911957,
       0.3010466694831848,
       0.30024033784866333,
       0.2993691563606262,
       0.29818934202194214,
       0.2973678708076477,
       0.2959638237953186,
       0.29522716999053955,
       0.2941240966320038,
       0.2926914393901825,
       0.2915056347846985,
       0.290192574262619,
       0.28879544138908386,
       0.28751498460769653,
       0.2864772081375122,
       0.28463757038116455,
       0.2827463746070862,
       0.2808762788772583,
       0.27937689423561096,
       0.2773389518260956,
       0.27524787187576294,
       0.27268898487091064,
       0.27064672112464905,
       0.2680748701095581,
       0.26491889357566833,
       0.26183372735977173,
       0.2589223086833954,
       0.2555069923400879,
       0.2519504725933075,
       0.2487923502922058,
       0.24401022493839264,
       0.2396436482667923,
       0.23545178771018982,
       0.2304432988166809,
       0.22524623572826385,
       0.22030742466449738,
       0.21497653424739838,
       0.2104322463274002]},
     'relu': {'initial_loss': 0.680274486541748,
      'train_acc': 0.9649999737739563,
      'test_acc': 0.9750000238418579,
      'losses': [0.680274486541748,
       0.6178063750267029,
       0.559501051902771,
       0.49664759635925293,
       0.43135467171669006,
       0.3772669732570648,
       0.33878928422927856,
       0.31630074977874756,
       0.3022221624851227,
       0.29443609714508057,
       0.28907063603401184,
       0.28567075729370117,
       0.28119948506355286,
       0.2776472270488739,
       0.2744244635105133,
       0.2700521647930145,
       0.266145259141922,
       0.26155078411102295,
       0.2573930323123932,
       0.252228707075119,
       0.24649390578269958,
       0.24091824889183044,
       0.23550474643707275,
       0.2287750393152237,
       0.2225928157567978,
       0.21646851301193237,
       0.2095731943845749,
       0.2034284770488739,
       0.19666534662246704,
       0.18999063968658447,
       0.1836107075214386,
       0.17760276794433594,
       0.1710367351770401,
       0.1648867279291153,
       0.15805365145206451,
       0.15237443149089813,
       0.1469523012638092,
       0.14215093851089478,
       0.13707281649112701,
       0.13215863704681396,
       0.12781718373298645,
       0.12366006523370743,
       0.11954250186681747,
       0.11563867330551147,
       0.11312300711870193,
       0.1096668615937233,
       0.10733821243047714,
       0.10432638227939606,
       0.1023118793964386,
       0.10003919899463654]}}

    plt.figure()
    for name, res in results_keras.items():
        plt.plot(res["losses"], label=name)

    plt.xlabel("Epoch")
    plt.ylabel("Training Loss")
    plt.title("Keras: Loss Curves for Different Activations")
    plt.legend()
    plt.grid(True)
    plt.show()

[]

    keras_table = pd.DataFrame([
        {
            "Activation": name,
            "Initial Loss": round(res["initial_loss"], 4),
            "Final Train Accuracy": round(res["train_acc"], 4),
            "Final Test Accuracy": round(res["test_acc"], 4)
        }
        for name, res in results_keras.items()
    ])

    keras_table

    {"summary":"{\n  \"name\": \"keras_table\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"Activation\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"sigmoid\",\n          \"tanh\",\n          \"relu\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Initial Loss\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.18019991675913732,\n        \"min\": 0.6705,\n        \"max\": 0.9874,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.6705,\n          0.9874,\n          0.6803\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Final Train Accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04750592945447265,\n        \"min\": 0.87,\n        \"max\": 0.965,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.87,\n          0.9162,\n          0.965\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Final Test Accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05499999999999999,\n        \"min\": 0.865,\n        \"max\": 0.975,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.865,\n          0.92,\n          0.975\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}","type":"dataframe","variable_name":"keras_table"}

##EXERCISE 2 - Effect of Activation Function on Convergence Speed

    for name, res in results_keras.items():
        print(
            name,
            "Converges at epoch:",
            convergence_epoch(res["losses"]),
            "| Curve:",
            "Smooth" if name != "relu" else "Fast"
        )

    sigmoid Converges at epoch: 29 | Curve: Smooth
    tanh Converges at epoch: None | Curve: Smooth
    relu Converges at epoch: None | Curve: Fast

##EXERCISE 3

    lr_results_keras = {}

    for act in activations_keras:
        for lr in [0.0005, 0.01]:
            key = f"{act}_lr_{lr}"
            lr_results_keras[key] = train_keras(act, lr)

    lr_results_keras

    {'sigmoid_lr_0.0005': {'initial_loss': 0.7017185688018799,
      'train_acc': 0.8675000071525574,
      'test_acc': 0.8600000143051147,
      'losses': [0.7017185688018799,
       0.6859865784645081,
       0.6750638484954834,
       0.6668707132339478,
       0.6588138341903687,
       0.6508851647377014,
       0.642061173915863,
       0.6332791447639465,
       0.6236253976821899,
       0.6135990023612976,
       0.6029778122901917,
       0.5915240049362183,
       0.5793310403823853,
       0.5677757859230042,
       0.5539581775665283,
       0.5404142737388611,
       0.5265676379203796,
       0.5123973488807678,
       0.49808797240257263,
       0.48391029238700867,
       0.470093697309494,
       0.4564240276813507,
       0.4430578947067261,
       0.43034571409225464,
       0.4185675382614136,
       0.40685683488845825,
       0.39662230014801025,
       0.38691461086273193,
       0.3778708279132843,
       0.36957114934921265,
       0.36219409108161926,
       0.35531917214393616,
       0.34959137439727783,
       0.34402230381965637,
       0.3391064703464508,
       0.3347327411174774,
       0.3309890627861023,
       0.32742542028427124,
       0.3245173394680023,
       0.32195013761520386,
       0.31922176480293274,
       0.3172176480293274,
       0.3151811361312866,
       0.3136094808578491,
       0.31239521503448486,
       0.3105968236923218,
       0.3093213737010956,
       0.30823639035224915,
       0.30731481313705444,
       0.3063696026802063]},
     'sigmoid_lr_0.01': {'initial_loss': 0.6736859679222107,
      'train_acc': 0.9574999809265137,
      'test_acc': 0.9649999737739563,
      'losses': [0.6736859679222107,
       0.5418462157249451,
       0.3560357689857483,
       0.3113963305950165,
       0.3091520071029663,
       0.3073919415473938,
       0.30758804082870483,
       0.3029361665248871,
       0.3060041666030884,
       0.30375903844833374,
       0.3000004291534424,
       0.30330830812454224,
       0.3017672598361969,
       0.30092644691467285,
       0.2988254725933075,
       0.2992394268512726,
       0.3043128252029419,
       0.30394721031188965,
       0.29710158705711365,
       0.2992630898952484,
       0.2953322231769562,
       0.29643797874450684,
       0.2974812686443329,
       0.2954227030277252,
       0.2977684736251831,
       0.29900774359703064,
       0.29726797342300415,
       0.29852452874183655,
       0.29497379064559937,
       0.29496312141418457,
       0.2944200038909912,
       0.2911101281642914,
       0.28868669271469116,
       0.28433361649513245,
       0.2804896831512451,
       0.27431952953338623,
       0.26877862215042114,
       0.25328919291496277,
       0.23893187940120697,
       0.2254340499639511,
       0.21561722457408905,
       0.190033420920372,
       0.1805855929851532,
       0.16883522272109985,
       0.151124969124794,
       0.14446978271007538,
       0.1377948373556137,
       0.1310669183731079,
       0.1272410750389099,
       0.12970024347305298]},
     'tanh_lr_0.0005': {'initial_loss': 0.6826946139335632,
      'train_acc': 0.8862500190734863,
      'test_acc': 0.8849999904632568,
      'losses': [0.6826946139335632,
       0.5892035961151123,
       0.5121762752532959,
       0.4531663656234741,
       0.40787652134895325,
       0.3747517466545105,
       0.35172712802886963,
       0.33526480197906494,
       0.32423171401023865,
       0.3164542615413666,
       0.31134217977523804,
       0.30747103691101074,
       0.30457207560539246,
       0.3025926649570465,
       0.30140653252601624,
       0.30005374550819397,
       0.299249529838562,
       0.2984314560890198,
       0.29784926772117615,
       0.2971956729888916,
       0.2966700494289398,
       0.29617300629615784,
       0.2957015037536621,
       0.2951093316078186,
       0.29450303316116333,
       0.29417863488197327,
       0.29344362020492554,
       0.2928989827632904,
       0.2922039330005646,
       0.29161185026168823,
       0.2910729646682739,
       0.29069745540618896,
       0.28977441787719727,
       0.28885409235954285,
       0.2880454361438751,
       0.28708547353744507,
       0.28634506464004517,
       0.2854144871234894,
       0.2851261794567108,
       0.28332793712615967,
       0.28229057788848877,
       0.28124117851257324,
       0.2799972593784332,
       0.27896052598953247,
       0.2776227295398712,
       0.2761583924293518,
       0.27473244071006775,
       0.27344217896461487,
       0.27158045768737793,
       0.2704351544380188]},
     'tanh_lr_0.01': {'initial_loss': 0.3950437605381012,
      'train_acc': 0.9737499952316284,
      'test_acc': 0.9950000047683716,
      'losses': [0.3950437605381012,
       0.29620304703712463,
       0.273433119058609,
       0.23575955629348755,
       0.19486474990844727,
       0.1508185863494873,
       0.11551227420568466,
       0.1019354835152626,
       0.09615850448608398,
       0.09617770463228226,
       0.09402599930763245,
       0.08196239173412323,
       0.08390140533447266,
       0.08838780224323273,
       0.09205988794565201,
       0.08377847820520401,
       0.0772775411605835,
       0.07854507118463516,
       0.08035659044981003,
       0.08223585039377213,
       0.08236397057771683,
       0.08387436717748642,
       0.09560322016477585,
       0.07406377047300339,
       0.07832799106836319,
       0.08598560094833374,
       0.09054268896579742,
       0.07947738468647003,
       0.088994599878788,
       0.07620619237422943,
       0.08223037421703339,
       0.08013119548559189,
       0.0861714631319046,
       0.07899809628725052,
       0.0835520327091217,
       0.0931786373257637,
       0.08403491228818893,
       0.07579140365123749,
       0.08450596779584885,
       0.08120709657669067,
       0.0772508755326271,
       0.09699108451604843,
       0.08423198759555817,
       0.08083774894475937,
       0.07406473904848099,
       0.07386881858110428,
       0.08232362568378448,
       0.08004117012023926,
       0.0781087651848793,
       0.07507088780403137]},
     'relu_lr_0.0005': {'initial_loss': 0.7017000317573547,
      'train_acc': 0.8987500071525574,
      'test_acc': 0.8949999809265137,
      'losses': [0.7017000317573547,
       0.6556928753852844,
       0.6139771938323975,
       0.5726392865180969,
       0.5336021184921265,
       0.4963749647140503,
       0.4612942636013031,
       0.42926308512687683,
       0.4006825387477875,
       0.3747582733631134,
       0.3523574471473694,
       0.3352298438549042,
       0.32221853733062744,
       0.31292927265167236,
       0.305903822183609,
       0.3008505702018738,
       0.2967923879623413,
       0.2937052547931671,
       0.29110297560691833,
       0.28898704051971436,
       0.28633952140808105,
       0.28473609685897827,
       0.2828768193721771,
       0.28146103024482727,
       0.27963390946388245,
       0.2784366309642792,
       0.2764231264591217,
       0.27473536133766174,
       0.2729489207267761,
       0.27137330174446106,
       0.2692774534225464,
       0.2676122188568115,
       0.26529574394226074,
       0.26340651512145996,
       0.2613866329193115,
       0.25957995653152466,
       0.25755128264427185,
       0.25528669357299805,
       0.25310641527175903,
       0.25119760632514954,
       0.24898599088191986,
       0.24675293266773224,
       0.24416963756084442,
       0.24188640713691711,
       0.23950201272964478,
       0.2369876652956009,
       0.23447702825069427,
       0.23175424337387085,
       0.22939452528953552,
       0.22643472254276276]},
     'relu_lr_0.01': {'initial_loss': 0.47053128480911255,
      'train_acc': 0.9700000286102295,
      'test_acc': 0.9850000143051147,
      'losses': [0.47053128480911255,
       0.28475692868232727,
       0.2566731870174408,
       0.2188577651977539,
       0.17490509152412415,
       0.13247151672840118,
       0.10902602970600128,
       0.09668895602226257,
       0.0948912501335144,
       0.0918084904551506,
       0.08461043983697891,
       0.08282583206892014,
       0.08176671713590622,
       0.08168907463550568,
       0.08436504006385803,
       0.0830240249633789,
       0.0835031121969223,
       0.07736671715974808,
       0.07763542979955673,
       0.07874148339033127,
       0.07342058420181274,
       0.07550299912691116,
       0.07103012502193451,
       0.07364492863416672,
       0.07279510796070099,
       0.07189174741506577,
       0.06816810369491577,
       0.0748516395688057,
       0.0684385895729065,
       0.07501200586557388,
       0.08508763462305069,
       0.08177310973405838,
       0.0667344257235527,
       0.0628778338432312,
       0.07779953628778458,
       0.07327472418546677,
       0.08139963448047638,
       0.08381962031126022,
       0.07208923995494843,
       0.06808353960514069,
       0.06620556116104126,
       0.06608092039823532,
       0.07568274438381195,
       0.06630083918571472,
       0.07005829364061356,
       0.06993739306926727,
       0.07614894211292267,
       0.0688476413488388,
       0.06661255657672882,
       0.07099203765392303]}}

    keras_lr_table = pd.DataFrame([
        {
            "Activation + LR": key,
            "Initial Loss": round(res["initial_loss"], 4),
            "Final Train Accuracy": round(res["train_acc"], 4),
            "Final Test Accuracy": round(res["test_acc"], 4)
        }
        for key, res in lr_results_keras.items()
    ])

    keras_lr_table

    {"summary":"{\n  \"name\": \"keras_lr_table\",\n  \"rows\": 6,\n  \"fields\": [\n    {\n      \"column\": \"Activation + LR\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"sigmoid_lr_0.0005\",\n          \"sigmoid_lr_0.01\",\n          \"relu_lr_0.01\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Initial Loss\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.1353848797564435,\n        \"min\": 0.395,\n        \"max\": 0.7017,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.6737,\n          0.4705,\n          0.6827\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Final Train Accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04677801477902483,\n        \"min\": 0.8675,\n        \"max\": 0.9737,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0.8675,\n          0.9575,\n          0.97\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Final Test Accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05765558660413288,\n        \"min\": 0.86,\n        \"max\": 0.995,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0.86,\n          0.965,\n          0.985\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}","type":"dataframe","variable_name":"keras_lr_table"}
